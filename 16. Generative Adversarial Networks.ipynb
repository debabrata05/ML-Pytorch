{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836a1c99-70d2-4d98-9210-21739e9d7290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "print(\"GPU Available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea82a1fb-5b9d-4bbf-aa4c-bb95c9b0c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0f67c2-4b91-4bd6-90f3-69026725e50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define a function for the generator:\n",
    "def make_generator_network(input_size=20, num_hidden_layers=1, num_hidden_units=100, num_output_units=784):\n",
    "    model = nn.Sequential()\n",
    "    for i in range(num_hidden_layers):\n",
    "        model.add_module(f'fc_g{i}', nn.Linear(input_size, num_hidden_units)) \n",
    "        model.add_module(f'relu_g{i}', nn.LeakyReLU())     \n",
    "        input_size = num_hidden_units\n",
    "    model.add_module(f'fc_g{num_hidden_layers}', nn.Linear(input_size, num_output_units))   \n",
    "    model.add_module('tanh_g', nn.Tanh())      \n",
    "    return model\n",
    "\n",
    "## define a function for the discriminator:\n",
    "def make_discriminator_network(input_size, num_hidden_layers=1, num_hidden_units=100, num_output_units=1):\n",
    "    model = nn.Sequential()\n",
    "    for i in range(num_hidden_layers):\n",
    "        model.add_module(f'fc_d{i}', nn.Linear(input_size, num_hidden_units, bias=False)) \n",
    "        model.add_module(f'relu_d{i}', nn.LeakyReLU())  \n",
    "        model.add_module('dropout', nn.Dropout(p=0.5))\n",
    "        input_size = num_hidden_units\n",
    "    model.add_module(f'fc_d{num_hidden_layers}', nn.Linear(input_size, num_output_units))   \n",
    "    model.add_module('sigmoid', nn.Sigmoid())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5914ff64-7af5-4064-ba80-9eca4740f93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (28, 28)\n",
    "z_size = 20\n",
    "\n",
    "gen_hidden_layers = 1\n",
    "gen_hidden_size = 100\n",
    "disc_hidden_layers = 1\n",
    "disc_hidden_size = 100\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "gen_model = make_generator_network(input_size=z_size, \n",
    "                                   num_hidden_layers=gen_hidden_layers, \n",
    "                                   num_hidden_units=gen_hidden_size, \n",
    "                                   num_output_units=np.prod(image_size))\n",
    "\n",
    "disc_model = make_discriminator_network(input_size=np.prod(image_size), \n",
    "                                        num_hidden_layers=disc_hidden_layers, \n",
    "                                        num_hidden_units=disc_hidden_size)\n",
    "\n",
    "print(gen_model)\n",
    "print(disc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea814472-af44-40b0-b45c-02253775db85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision \n",
    "from torchvision import transforms \n",
    "\n",
    "image_path = './'\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=(0.5), std=(0.5))])\n",
    "mnist_dataset = torchvision.datasets.MNIST(root=image_path, train=True, transform=transform, download=False)\n",
    "\n",
    "example, label = next(iter(mnist_dataset))\n",
    "print(f'Min: {example.min()} Max: {example.max()}')\n",
    "print(example.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f74e09f-fd56-48af-aa08-758f9fe5ca0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_noise(batch_size, z_size, mode_z):\n",
    "    if mode_z == 'uniform':\n",
    "        input_z = torch.rand(batch_size, z_size)*2 - 1 \n",
    "    elif mode_z == 'normal':\n",
    "        input_z = torch.randn(batch_size, z_size)\n",
    "    return input_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a272ed-c0aa-4cbe-aaec-a293a003e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(mnist_dataset, batch_size, shuffle=False)\n",
    "input_real, label = next(iter(dataloader))\n",
    "input_real = input_real.view(batch_size, -1)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "mode_z = 'uniform'  # 'uniform' vs. 'normal'\n",
    "input_z = create_noise(batch_size, z_size, mode_z)\n",
    "\n",
    "print('input-z -- shape:', input_z.shape)\n",
    "print('input-real -- shape:', input_real.shape)\n",
    "\n",
    "g_output = gen_model(input_z)\n",
    "print('Output of G -- shape:', g_output.shape)\n",
    "\n",
    "d_proba_real = disc_model(input_real)\n",
    "d_proba_fake = disc_model(g_output)\n",
    "print('Disc. (real) -- shape:', d_proba_real.shape)\n",
    "print('Disc. (fake) -- shape:', d_proba_fake.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2c6867-5474-45ba-84a4-b4be5e5af4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "## Loss for the Generator\n",
    "g_labels_real = torch.ones_like(d_proba_fake)\n",
    "g_loss = loss_fn(d_proba_fake, g_labels_real)\n",
    "print(f'Generator Loss: {g_loss:.4f}')\n",
    "\n",
    "## Loss for the Discriminator\n",
    "d_labels_real = torch.ones_like(d_proba_real)\n",
    "d_labels_fake = torch.zeros_like(d_proba_fake)\n",
    "\n",
    "d_loss_real = loss_fn(d_proba_real, d_labels_real)\n",
    "d_loss_fake = loss_fn(d_proba_fake, d_labels_fake)\n",
    "print(f'Discriminator Losses: Real {d_loss_real:.4f} Fake {d_loss_fake:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d02ee1e-af24-420f-9771-7ec4ba6e3de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "## Set up the dataset\n",
    "mnist_dl = DataLoader(mnist_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    " \n",
    "## Set up the models\n",
    "gen_model = make_generator_network(input_size=z_size, \n",
    "                                   num_hidden_layers=gen_hidden_layers, \n",
    "                                   num_hidden_units=gen_hidden_size, \n",
    "                                   num_output_units=np.prod(image_size)).to(device)\n",
    " \n",
    "disc_model = make_discriminator_network(input_size=np.prod(image_size), \n",
    "                                        num_hidden_layers=disc_hidden_layers,\n",
    "                                        num_hidden_units=disc_hidden_size).to(device)\n",
    " \n",
    "## Loss function and optimizers:\n",
    "loss_fn = nn.BCELoss()\n",
    "g_optimizer = torch.optim.Adam(gen_model.parameters())\n",
    "d_optimizer = torch.optim.Adam(disc_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdd0a53-0445-45c0-b66b-ed65bc18f177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_train(x):\n",
    "    disc_model.zero_grad()\n",
    "\n",
    "    # Train discriminator with a real batch\n",
    "    batch_size = x.size(0)\n",
    "    x = x.view(batch_size, -1).to(device)\n",
    "    d_labels_real = torch.ones(batch_size, 1, device=device)\n",
    "\n",
    "    d_proba_real = disc_model(x)\n",
    "    d_loss_real = loss_fn(d_proba_real, d_labels_real)\n",
    "\n",
    "    # Train discriminator on a fake batch\n",
    "    input_z = create_noise(batch_size, z_size, mode_z).to(device)\n",
    "    g_output = gen_model(input_z)\n",
    "    \n",
    "    d_proba_fake = disc_model(g_output)\n",
    "    d_labels_fake = torch.zeros(batch_size, 1, device=device)\n",
    "    d_loss_fake = loss_fn(d_proba_fake, d_labels_fake)\n",
    "\n",
    "    # gradient backprop & optimize ONLY D's parameters\n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "    d_loss.backward()\n",
    "    d_optimizer.step()\n",
    "  \n",
    "    return d_loss.data.item(), d_proba_real.detach(), d_proba_fake.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08f010b-905d-4ae3-93fd-de5b6d0207cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_train(x):\n",
    "    gen_model.zero_grad()\n",
    "    \n",
    "    batch_size = x.size(0)\n",
    "    input_z = create_noise(batch_size, z_size, mode_z).to(device)\n",
    "    g_labels_real = torch.ones(batch_size, 1, device=device)\n",
    "\n",
    "    g_output = gen_model(input_z)\n",
    "    d_proba_fake = disc_model(g_output)\n",
    "    g_loss = loss_fn(d_proba_fake, g_labels_real)\n",
    "\n",
    "    # gradient backprop & optimize ONLY G's parameters\n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "        \n",
    "    return g_loss.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf3b9c5-9468-4d0e-bea8-59b671d6f744",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_z = create_noise(batch_size, z_size, mode_z).to(device)\n",
    "\n",
    "def create_samples(g_model, input_z):\n",
    "    g_output = g_model(input_z)\n",
    "    images = torch.reshape(g_output, (batch_size, *image_size))    \n",
    "    return (images+1)/2.0\n",
    "\n",
    "epoch_samples = []\n",
    "\n",
    "all_d_losses = []\n",
    "all_g_losses = []\n",
    "\n",
    "all_d_real = []\n",
    "all_d_fake = []\n",
    "\n",
    "num_epochs = 100\n",
    "torch.manual_seed(1)\n",
    "for epoch in range(1, num_epochs+1):           \n",
    "    d_losses, g_losses = [], []\n",
    "    d_vals_real, d_vals_fake = [], []\n",
    "    for i, (x, _) in enumerate(mnist_dl):\n",
    "        d_loss, d_proba_real, d_proba_fake = d_train(x)\n",
    "        d_losses.append(d_loss)\n",
    "        g_losses.append(g_train(x))\n",
    "        \n",
    "        d_vals_real.append(d_proba_real.mean().cpu())\n",
    "        d_vals_fake.append(d_proba_fake.mean().cpu())\n",
    "        \n",
    "    all_d_losses.append(torch.tensor(d_losses).mean())\n",
    "    all_g_losses.append(torch.tensor(g_losses).mean())\n",
    "    all_d_real.append(torch.tensor(d_vals_real).mean())\n",
    "    all_d_fake.append(torch.tensor(d_vals_fake).mean())\n",
    "    print(f'Epoch {epoch:03d} | Avg Losses >>'\n",
    "          f' G/D {all_g_losses[-1]:.4f}/{all_d_losses[-1]:.4f}'\n",
    "          f' [D-Real: {all_d_real[-1]:.4f} D-Fake: {all_d_fake[-1]:.4f}]')\n",
    "    epoch_samples.append(\n",
    "        create_samples(gen_model, fixed_z).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7d8025-6e35-4dc9-a10d-53cb2a97d93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "\n",
    "## Plotting the losses\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    " \n",
    "plt.plot(all_g_losses, label='Generator loss')\n",
    "half_d_losses = [all_d_loss/2 for all_d_loss in all_d_losses]\n",
    "plt.plot(half_d_losses, label='Discriminator loss')\n",
    "plt.legend(fontsize=20)\n",
    "ax.set_xlabel('Iteration', size=15)\n",
    "ax.set_ylabel('Loss', size=15)\n",
    "\n",
    "## Plotting the outputs of the discriminator\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "plt.plot(all_d_real, label=r'Real: $D(\\mathbf{x})$')\n",
    "plt.plot(all_d_fake, label=r'Fake: $D(G(\\mathbf{z}))$')\n",
    "plt.legend(fontsize=20)\n",
    "ax.set_xlabel('Iteration', size=15)\n",
    "ax.set_ylabel('Discriminator output', size=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c47b94c-81f2-442e-acc3-6a759ef34a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_epochs = [1, 2, 4, 10, 50, 100]\n",
    "fig = plt.figure(figsize=(10, 14))\n",
    "for i,e in enumerate(selected_epochs):\n",
    "    for j in range(5):\n",
    "        ax = fig.add_subplot(6, 5, i*5+j+1)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if j == 0:\n",
    "            ax.text(-0.06, 0.5, f'Epoch {e}', \n",
    "                    rotation=90, size=18, color='red', \n",
    "                    horizontalalignment='right', \n",
    "                    verticalalignment='center', \n",
    "                    transform=ax.transAxes)\n",
    "        \n",
    "        image = epoch_samples[e-1][j]\n",
    "        ax.imshow(image, cmap='gray_r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f41d8a-de3c-486a-b659-055343cf8789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def distance(X, Y, sqrt):\n",
    "    nX = X.size(0)\n",
    "    nY = Y.size(0)\n",
    "    X = X.view(nX,-1).cuda()\n",
    "    X2 = (X*X).sum(1).resize_(nX,1)\n",
    "    Y = Y.view(nY,-1).cuda()\n",
    "    Y2 = (Y*Y).sum(1).resize_(nY,1)\n",
    "\n",
    "    M = torch.zeros(nX, nY)\n",
    "    M.copy_(X2.expand(nX,nY) + Y2.expand(nY,nX).transpose(0,1) - 2*torch.mm(X,Y.transpose(0,1)))\n",
    "\n",
    "    del X, X2, Y, Y2\n",
    "    \n",
    "    if sqrt:\n",
    "        M = ((M+M.abs())/2).sqrt()\n",
    "    \n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a07fd08-fee3-40b8-9371-28c9fa55db34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmd(Mxx, Mxy, Myy, sigma) :\n",
    "    scale = Mxx.mean()\n",
    "    Mxx = torch.exp(-Mxx/(scale*2*sigma*sigma))\n",
    "    Mxy = torch.exp(-Mxy/(scale*2*sigma*sigma))\n",
    "    Myy = torch.exp(-Myy/(scale*2*sigma*sigma))\n",
    "    a = Mxx.mean()+Myy.mean()-2*Mxy.mean()\n",
    "    mmd = math.sqrt(max(a, 0))\n",
    "\n",
    "    return mmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccc119f-403c-4a76-b96f-6571ef730cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score(fake, real , k=1, sigma=1, sqrt=True):\n",
    "    Mxx = distance(real, real, False)\n",
    "    Mxy = distance(real, fake, False)\n",
    "    Myy = distance(fake, fake, False)\n",
    "\n",
    "    print(mmd(Mxx, Mxy, Myy, sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad577424-e2a7-4b93-8563-0b71735a8236",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_dl = DataLoader(mnist_dataset, batch_size=10000, shuffle=True, drop_last=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a16d0f-732e-448b-bf75-13abfabf7fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_image = next(iter(whole_dl))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee65535-7113-4832-ab78-a278ea11debd",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_score(torch.from_numpy(epoch_samples[-1]), real_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee459012-c203-4edc-9047-9aa905401a41",
   "metadata": {},
   "source": [
    "### Train the DCGAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101c8a34-8f28-4980-b067-6e345f1f5573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(\"GPU Available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535654bf-acd0-4c71-a81b-993110f85ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e911a9a9-0e86-40f2-b68e-7a6cde8fa9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision \n",
    "from torchvision import transforms \n",
    "\n",
    "\n",
    "image_path = './'\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=(0.5), std=(0.5))])\n",
    "mnist_dataset = torchvision.datasets.MNIST(root=image_path, train=True, transform=transform, download=True)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "## Set up the dataset\n",
    "from torch.utils.data import DataLoader\n",
    "mnist_dl = DataLoader(mnist_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265eb2ca-c9a7-428e-935a-25ecc5be25f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_network(input_size, n_filters):\n",
    "    model = nn.Sequential(nn.ConvTranspose2d(input_size, n_filters*4, 4, 1, 0, bias=False), \n",
    "                          nn.BatchNorm2d(n_filters*4), \n",
    "                          nn.LeakyReLU(0.2),\n",
    "\n",
    "                          nn.ConvTranspose2d(n_filters*4, n_filters*2, 3, 2, 1, bias=False),\n",
    "                          nn.BatchNorm2d(n_filters*2),\n",
    "                          nn.LeakyReLU(0.2),\n",
    "\n",
    "                          nn.ConvTranspose2d(n_filters*2, n_filters, 4, 2, 1, bias=False),\n",
    "                          nn.BatchNorm2d(n_filters),\n",
    "                          nn.LeakyReLU(0.2),\n",
    "                          \n",
    "                          nn.ConvTranspose2d(n_filters, 1, 4, 2, 1, bias=False),\n",
    "                          nn.Tanh())\n",
    "    return model\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, n_filters):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(nn.Conv2d(1, n_filters, 4, 2, 1, bias=False),\n",
    "                                     nn.LeakyReLU(0.2),\n",
    "\n",
    "                                     nn.Conv2d(n_filters, n_filters*2, 4, 2, 1, bias=False),\n",
    "                                     nn.BatchNorm2d(n_filters * 2),\n",
    "                                     nn.LeakyReLU(0.2),\n",
    "\n",
    "                                     nn.Conv2d(n_filters*2, n_filters*4, 3, 2, 1, bias=False),\n",
    "                                     nn.BatchNorm2d(n_filters*4),\n",
    "                                     nn.LeakyReLU(0.2),\n",
    "\n",
    "                                     nn.Conv2d(n_filters*4, 1, 4, 1, 0, bias=False),\n",
    "                                     nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = self.network(input)\n",
    "        return output.view(-1, 1).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec0a3c2-a389-49d6-abcd-7a6cca69bab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_size = 100\n",
    "image_size = (28, 28)\n",
    "n_filters = 32 \n",
    "gen_model = make_generator_network(z_size, n_filters).to(device)  \n",
    "print(gen_model)\n",
    "disc_model = Discriminator(n_filters).to(device)     \n",
    "print(disc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f976db-ff61-4611-a7ac-519fedadba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()\n",
    "g_optimizer = torch.optim.Adam(gen_model.parameters(), 0.0003)\n",
    "d_optimizer = torch.optim.Adam(disc_model.parameters(), 0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da45a44f-6d58-4f44-ad00-653578ae9733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_noise(batch_size, z_size, mode_z):\n",
    "    if mode_z == 'uniform':\n",
    "        input_z = torch.rand(batch_size, z_size, 1, 1)*2 - 1 \n",
    "    elif mode_z == 'normal':\n",
    "        input_z = torch.randn(batch_size, z_size, 1, 1)\n",
    "    return input_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9035a624-5c9c-485a-bc20-c049306b765d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_train(x):\n",
    "    disc_model.zero_grad()\n",
    "\n",
    "    # Train discriminator with a real batch\n",
    "    batch_size = x.size(0)\n",
    "    x = x.to(device)\n",
    "    d_labels_real = torch.ones(batch_size, 1, device=device)\n",
    "\n",
    "    d_proba_real = disc_model(x)\n",
    "    d_loss_real = loss_fn(d_proba_real, d_labels_real)\n",
    "\n",
    "    # Train discriminator on a fake batch\n",
    "    input_z = create_noise(batch_size, z_size, mode_z).to(device)\n",
    "    g_output = gen_model(input_z)\n",
    "    \n",
    "    d_proba_fake = disc_model(g_output)\n",
    "    d_labels_fake = torch.zeros(batch_size, 1, device=device)\n",
    "    d_loss_fake = loss_fn(d_proba_fake, d_labels_fake)\n",
    "\n",
    "    # gradient backprop & optimize ONLY D's parameters\n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "    d_loss.backward()\n",
    "    d_optimizer.step()\n",
    "  \n",
    "    return d_loss.data.item(), d_proba_real.detach(), d_proba_fake.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caeb496-fdb0-4e03-83f8-64d67908e7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_train(x):\n",
    "    gen_model.zero_grad()\n",
    "    \n",
    "    batch_size = x.size(0)\n",
    "    input_z = create_noise(batch_size, z_size, mode_z).to(device)\n",
    "    g_labels_real = torch.ones((batch_size, 1), device=device)\n",
    "\n",
    "    g_output = gen_model(input_z)\n",
    "    d_proba_fake = disc_model(g_output)\n",
    "    g_loss = loss_fn(d_proba_fake, g_labels_real)\n",
    "\n",
    "    # gradient backprop & optimize ONLY G's parameters\n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "        \n",
    "    return g_loss.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84610220-6db0-415d-85a9-7d0d556f841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_z = 'uniform'\n",
    "fixed_z = create_noise(batch_size, z_size, mode_z).to(device)\n",
    "\n",
    "def create_samples(g_model, input_z):\n",
    "    g_output = g_model(input_z)\n",
    "    images = torch.reshape(g_output, (batch_size, *image_size))    \n",
    "    return (images+1)/2.0\n",
    "\n",
    "epoch_samples = []\n",
    "\n",
    "num_epochs = 100\n",
    "torch.manual_seed(1)\n",
    "\n",
    "for epoch in range(1, num_epochs+1):    \n",
    "    gen_model.train()\n",
    "    d_losses, g_losses = [], []\n",
    "    for i, (x, _) in enumerate(mnist_dl):\n",
    "        d_loss, d_proba_real, d_proba_fake = d_train(x)\n",
    "        d_losses.append(d_loss)\n",
    "        g_losses.append(g_train(x))\n",
    " \n",
    "    print(f'Epoch {epoch:03d} | Avg Losses >>'\n",
    "          f' G/D {torch.FloatTensor(g_losses).mean():.4f}'\n",
    "          f'/{torch.FloatTensor(d_losses).mean():.4f}')\n",
    "    gen_model.eval()\n",
    "    epoch_samples.append(\n",
    "        create_samples(gen_model, fixed_z).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cf045b-f7c6-4a43-8f5a-70d2e8b0a44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_epochs = [1, 2, 4, 10, 50, 100]\n",
    "fig = plt.figure(figsize=(10, 14))\n",
    "for i,e in enumerate(selected_epochs):\n",
    "    for j in range(5):\n",
    "        ax = fig.add_subplot(6, 5, i*5+j+1)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if j == 0:\n",
    "            ax.text(-0.06, 0.5, f'Epoch {e}',\n",
    "                    rotation=90, size=18, color='red',\n",
    "                    horizontalalignment='right',\n",
    "                    verticalalignment='center', \n",
    "                    transform=ax.transAxes)\n",
    "        \n",
    "        image = epoch_samples[e-1][j]\n",
    "        ax.imshow(image, cmap='gray_r')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13906daa-fafa-4a7c-a1fe-dee56320e82b",
   "metadata": {},
   "source": [
    "### Implementing WGAN-GP to train the DCGAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f59fff-c2f4-4c01-992e-974099b74f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_network_wgan(input_size, n_filters):\n",
    "    model = nn.Sequential(nn.ConvTranspose2d(input_size, n_filters*4, 4, 1, 0, bias=False),\n",
    "                          nn.InstanceNorm2d(n_filters*4),\n",
    "                          nn.LeakyReLU(0.2),\n",
    "                          \n",
    "                          nn.ConvTranspose2d(n_filters*4, n_filters*2, 3, 2, 1, bias=False),\n",
    "                          nn.InstanceNorm2d(n_filters*2),\n",
    "                          nn.LeakyReLU(0.2),\n",
    "                          \n",
    "                          nn.ConvTranspose2d(n_filters*2, n_filters, 4, 2, 1, bias=False),\n",
    "                          nn.InstanceNorm2d(n_filters),\n",
    "                          nn.LeakyReLU(0.2),\n",
    "                          \n",
    "                          nn.ConvTranspose2d(n_filters, 1, 4, 2, 1, bias=False),\n",
    "                          nn.Tanh())\n",
    "    return model\n",
    "\n",
    "class DiscriminatorWGAN(nn.Module):\n",
    "    def __init__(self, n_filters):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(nn.Conv2d(1, n_filters, 4, 2, 1, bias=False),\n",
    "                                     nn.LeakyReLU(0.2),\n",
    "                                     \n",
    "                                     nn.Conv2d(n_filters, n_filters*2, 4, 2, 1, bias=False),\n",
    "                                     nn.InstanceNorm2d(n_filters * 2),\n",
    "                                     nn.LeakyReLU(0.2),\n",
    "                                     \n",
    "                                     nn.Conv2d(n_filters*2, n_filters*4, 3, 2, 1, bias=False),\n",
    "                                     nn.InstanceNorm2d(n_filters*4),\n",
    "                                     nn.LeakyReLU(0.2),\n",
    "                                     \n",
    "                                     nn.Conv2d(n_filters*4, 1, 4, 1, 0, bias=False),\n",
    "                                     nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = self.network(input)\n",
    "        return output.view(-1, 1).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04ac07e-4cce-4e3c-b97b-e8fc128e1eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model = make_generator_network_wgan(z_size, n_filters).to(device)  \n",
    "disc_model = DiscriminatorWGAN(n_filters).to(device)  \n",
    "\n",
    "g_optimizer = torch.optim.Adam(gen_model.parameters(), 0.0002)\n",
    "d_optimizer = torch.optim.Adam(disc_model.parameters(), 0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96b76b0-1444-44ab-acbf-81fa9d930ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import grad as torch_grad\n",
    "\n",
    "def gradient_penalty(real_data, generated_data):\n",
    "    batch_size = real_data.size(0)\n",
    "\n",
    "    # Calculate interpolation\n",
    "    alpha = torch.rand(real_data.shape[0], 1, 1, 1, requires_grad=True, device=device)\n",
    "    interpolated = alpha * real_data + (1 - alpha) * generated_data\n",
    "    \n",
    "    # Calculate probability of interpolated examples\n",
    "    proba_interpolated = disc_model(interpolated)\n",
    "\n",
    "    # Calculate gradients of probabilities with respect to examples\n",
    "    gradients = torch_grad(outputs=proba_interpolated, inputs=interpolated,\n",
    "                           grad_outputs=torch.ones(proba_interpolated.size(), device=device),\n",
    "                           create_graph=True,\n",
    "                           retain_graph=True)[0]\n",
    "\n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    gradients_norm = gradients.norm(2, dim=1)\n",
    "    return lambda_gp * ((gradients_norm - 1)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb0a976-06df-4a9b-8c2a-85ec43513ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_train_wgan(x):\n",
    "    disc_model.zero_grad()\n",
    "\n",
    "    batch_size = x.size(0)\n",
    "    x = x.to(device)\n",
    "\n",
    "    # Calculate probabilities on real and generated data\n",
    "    d_real = disc_model(x)\n",
    "    input_z = create_noise(batch_size, z_size, mode_z).to(device)\n",
    "    g_output = gen_model(input_z)\n",
    "    d_generated = disc_model(g_output)\n",
    "    d_loss = d_generated.mean() - d_real.mean() + gradient_penalty(x.data, g_output.data)\n",
    "    d_loss.backward()\n",
    "    d_optimizer.step()\n",
    "  \n",
    "    return d_loss.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b3de77-e547-4167-9f95-67bf769315c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_train_wgan(x):\n",
    "    gen_model.zero_grad()\n",
    "    \n",
    "    batch_size = x.size(0)\n",
    "    input_z = create_noise(batch_size, z_size, mode_z).to(device)\n",
    "    g_output = gen_model(input_z)\n",
    "    \n",
    "    d_generated = disc_model(g_output)\n",
    "    g_loss = -d_generated.mean()\n",
    "\n",
    "    # gradient backprop & optimize ONLY G's parameters\n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "        \n",
    "    return g_loss.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf0e2c6-17b9-4083-9088-371811f4b2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_samples_wgan = []\n",
    "lambda_gp = 10.0\n",
    "num_epochs = 100\n",
    "torch.manual_seed(1)\n",
    "critic_iterations = 5 \n",
    "\n",
    "for epoch in range(1, num_epochs+1):    \n",
    "    gen_model.train()\n",
    "    d_losses, g_losses = [], []\n",
    "    for i, (x, _) in enumerate(mnist_dl):\n",
    "        for _ in range(critic_iterations):\n",
    "            d_loss = d_train_wgan(x)\n",
    "        d_losses.append(d_loss)\n",
    "        g_losses.append(g_train_wgan(x))\n",
    " \n",
    "    print(f'Epoch {epoch:03d} | D Loss >>'\n",
    "          f' {torch.FloatTensor(d_losses).mean():.4f}')\n",
    "    gen_model.eval()\n",
    "    epoch_samples_wgan.append(\n",
    "        create_samples(gen_model, fixed_z).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fa9f00-ee53-4f5e-8d66-9544f4e95925",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_epochs = [1, 2, 4, 10, 50, 100]\n",
    "# selected_epochs = [1, 10, 20, 30, 50, 70]\n",
    "fig = plt.figure(figsize=(10, 14))\n",
    "for i,e in enumerate(selected_epochs):\n",
    "    for j in range(5):\n",
    "        ax = fig.add_subplot(6, 5, i*5+j+1)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if j == 0:\n",
    "            ax.text(-0.06, 0.5, f'Epoch {e}',\n",
    "                    rotation=90, size=18, color='red',\n",
    "                    horizontalalignment='right',\n",
    "                    verticalalignment='center', \n",
    "                    transform=ax.transAxes)\n",
    "        \n",
    "        image = epoch_samples_wgan[e-1][j]\n",
    "        ax.imshow(image, cmap='gray_r')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe0a014-c9c0-4c4e-9df0-396572ce901d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
